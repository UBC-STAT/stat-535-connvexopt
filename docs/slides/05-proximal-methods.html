<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>05 Proximal methods and SGD</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 535A" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 05 Proximal methods and SGD
### STAT 535A
### Daniel J. McDonald
### Last modified - 2021-01-25

---




## Last time


$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmin}[1]{\underset{#1}{\textrm{argmin}}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\indicator}{1}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\dom}[1]{\textrm{dom}\left(#1\right)}
\newcommand{\st}{\;\;\textrm{ s.t. }\;\;}
$$



Looked at unconstrained optimization 
`\begin{equation}
\min_x f(x)
\end{equation}`

.pull-left[
__Gradient descent__

-   Choose `\(x^{(0)}\)`

-   Iterate `\(x^{(k)} = x^{(k-1)} - \gamma_k\nabla f(x^{(k-1)})\)`

-   Stop sometime
]

.pull-right[
- Rates of convergence: "linear" means `\(\epsilon\)`-suboptimal in `\(O(1/\log(\epsilon))\)` iterations

- Improvements: acceleration, momentum, backtracking line search

- stopping criteria

]

--

Assumed `\(f\)` convex and **differentiable**

If not differentiable, used subgradient


---

## Today

**Some special cases**

1. Avoid subgradient method

2. `\(f(x) = g(x) + h(x)\)` with certain "nice" `\(g\)` and `\(h\)`

3. `\(f(x) = \sum_{i=1}^n f_i(x)\)`


---

## Composite functions

Subgradient can be slow

Suppose (still unconstrained)

$$
\min_x f(x) = g(x) + h(x)
$$

* `\(g(x)\)` is convex and differentiable
* `\(h(x)\)` is convex 

--

Taylor approximation to `\(f \longrightarrow\)` gradient descent

In GD, our update was (given `\(x\)` from previous iterate)

$$
`\begin{aligned}
x^{(k)} &amp;= \arg\min_z f(x^{(k-1)}) + \nabla f(x^{(k-1)})^\top (z - x^{(k-1)}) + \frac{1}{2\gamma}\norm{z - x^{(k-1)}}_2^2\\
&amp;= x^{(k-1)} - \gamma \nabla f(x^{(k-1)})
\end{aligned}`
$$

--

Instead, do Taylor approximation to `\(g\)` only

---

## Approximate `\(g\)` only

$$
`\begin{aligned}
x^{(k+1)} &amp;= \arg\min_z g(x^{(k)}) + \nabla g(x^{(k)})^\top (z - x^{(k)}) + \frac{1}{2\gamma}\norm{z - x^{(k)}}_2^2 + h(z)\\
&amp;= \arg\min_z \frac{1}{2\gamma} \norm{ z - (x^{(k)} - \gamma \nabla g(x^{(k)})) }_2^2 + h(z)
\end{aligned}`
$$

This has two parts:

* `\(\frac{1}{2\gamma} \norm{z - (x^{(k)} - \gamma \nabla g(x^{(k)})) }_2^2\)` means "stay close to the gradient update for `\(g\)`"

* `\(h(z)\)`, need to make this small too

* We're minimizing both over `\(z\)`.

--

Define the "proximal mapping/operator" of `\(h\)`:

$$
\textrm{prox}_{h,\gamma}(y) = \arg\min_z \frac{1}{2\gamma} \norm{ z - y }_2^2 + h(z)
$$


---

## Proximal gradient descent

Iterate:

1. `\(x^{g} = x^{(k)} - \gamma \nabla g(x^{(k)})\)` 

2. `\(x^{(k+1)} = \textrm{prox}_{h,\gamma}(x^g)\)` 

--

**What good did this do?**

* We swapped performing GD on `\(f\)` for GD on `\(g\)` plus some weird prox thing

* So unless, `\(\textrm{prox}_{h,\gamma}\)` is "simple", this is dumb.

--

Luckily, `\(\textrm{prox}_{h,\gamma}\)` **is** simple for many common `\(h\)`.

Often has closed form.

Furthermore, `\(g\)` can be complicated, as long as we can compute it's gradients.

---

## Example: LASSO

`$$\min_\beta \underbrace{\frac{1}{2}\norm{y-X\beta}_2^2}_g + \underbrace{\lambda\norm{\beta}_1}_h$$`

`$$\begin{aligned}
\textrm{prox}_{\lambda\norm{\cdot}_1,\gamma} (\beta) &amp;= \arg\min_z \norm{z-\beta}_2^2 + \lambda\norm{z}_1\\
&amp;=: S_{\lambda \gamma}(\beta)
\end{aligned}$$`

`$$[S_{\tau}(a)]_i = \textrm{sign}(a_i)(|a_i|-\tau)_+ = \begin{cases} 
a_i - \tau &amp; a_i &gt; \tau\\ 
0 &amp; |a_i| \leq \tau\\
a_i + \tau &amp; a_i &lt; -\tau
\end{cases}$$`


So the update becomes:

1. `\(\beta \leftarrow \beta + \gamma X^\top(y - X\beta)\)`
2. `\(\beta \leftarrow S_{\lambda \gamma}(\beta)\)`

This is called "Iterative soft thresholding" (ISTA)

---

## Projected gradient descent

$$
\min_{x\in C} f(x)
$$

- Incorporate the constraint into the objective using the indicator function:
`$$I_C(x) = \begin{cases} 0 &amp; x \in C\\ +\infty &amp; x \not\in C \end{cases}.$$`
- Call the indicator `\(h\)`, and use proximal GD
- Now `\(\textrm{prox}_t(x) = \arg\min_{z\in C} \norm{z-x}_2^2\)`
- This is just the Euclidean projection onto `\(C\)`


So the update becomes:

1. Use GD on `\(f\)`
2. Project onto `\(C\)`


---

## Constrained least squares

`$$\min_{l &lt; \beta &lt; u} \frac{1}{2n}\norm{y-X\beta}_2^2$$`

* You can perhaps imagine better ways to do this


.pull-left[

1. `\(\beta \leftarrow \beta + \frac{\gamma}{n}X^\top(y-X\beta)\)`

2. `\(\beta \leftarrow \Pi_{\{\beta : l &lt; \beta &lt; u\}} (\beta)\)`

Can show that

`$$[\Pi(z)]_i = \begin{cases}u_i &amp; z_i &gt; u_i \\ l_i &amp; z_i &lt; l_i \\ z_i &amp; \textrm{else} \end{cases}$$`

]

.pull-right[

```r
set.seed(12345)
n &lt;- 100
p &lt;- 10
X &lt;- matrix(rnorm(n*p), n)
bstar &lt;- c(rep(1,3),runif(p-6,-1,1),rep(-1,3))
y &lt;- X %*% bstar + rnorm(n)
Xy &lt;- crossprod(X,y) / n
XX &lt;- crossprod(X) / n
maxit &lt;- 200
gamma &lt;- .05
bhat &lt;- matrix(0, p, maxit)
grad &lt;- function(b) - (Xy - XX %*% b)
for (i in 2:maxit) {
  g &lt;- grad(bhat[,i-1])
  prop &lt;- bhat[,i-1] - gamma * g
  prop[prop &gt; 1] &lt;- 1 
  prop[prop &lt; -1] &lt;- -1
  bhat[,i] &lt;- prop
  if (sum(g^2) &lt; 1e-4) break
}
```

]

---

&lt;img src="rmd_gfx/05-proximal-methods/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

|       |   X1|   X2|   X3|   X4|   X5|   X6|   X7|    X8|    X9|   X10|
|:------|----:|----:|----:|----:|----:|----:|----:|-----:|-----:|-----:|
|bstar  | 1.00| 1.00| 1.00| 0.91| 0.24| 0.06| 0.78| -1.00| -1.00| -1.00|
|ProjGD | 0.90| 1.00| 0.99| 0.90| 0.16| 0.13| 0.67| -1.00| -1.00| -0.93|
|OLS    | 0.92| 1.19| 1.00| 0.89| 0.14| 0.08| 0.70| -1.22| -1.01| -0.92|

---

## Enhancements

Alternative formulation:

Define the "generalized gradient" as
`$$G_\gamma(x) = \frac{x - \textrm{prox}_{h,\gamma}(x - \gamma \nabla g(x))}{\gamma}$$`.

Then Proximal GD is (in 1 step):
`$$x^{(k)} = x^{(k)} - \gamma G_\gamma(x^{(k)})$$`

We use this definition for **backtracking** (on `\(g\)` only): shrink if
`$$g\left(x- \gamma G_\gamma(x)\right) &gt; g(x) - \gamma \nabla g(x)^\top G_\gamma(x) + \gamma \alpha \norm{G_\gamma(x)}_2^2$$`

* There are other formulations to avoid frequent evalutations of prox

For **acceleration**, need to express differently than last time (set `\(x^{(0)} = x^{(-1)}\)`):
1. `\(v \leftarrow x^{(k-1)} + \frac{k-2}{k+1} (x^{(k-1)} - x^{(k-2)})\)`
2. `\(x^{(k)} \leftarrow \textrm{prox}_{h,\gamma} (v - \gamma \nabla g(v))\)`

---

## Stochastic gradient descent

- Workhorse in Neural Networks
- Suppose `$$f(x) = \frac{1}{n} \sum_{i=1}^n f_i(x)$$`
- GD would use 
`$$x^{(k)} = x^{(k-1)} - \frac{\gamma}{n} \sum_{i=1}^n \nabla f_i(x^{(k-1)})$$`
- SGD uses
`$$x^{(k)} = x^{(k-1)} - \gamma \nabla f_{i_k}(x^{(k-1)})$$`
`\(i_k\in\{1,\ldots,n\}\)` is an index chosen at time `\(k\)`.

---

## Choosing the batches

Standard choices for `\(i_k\)`

1. Draw `\(i_k\sim \textrm{Unif}(\{1,\ldots,n\})\)`.
2. Set `\(i_k=1,\ldots,n,1,\ldots,n,1,\ldots,n,\ldots\)` (or a random permutation of these)

Most frequently update "mini-batches". 

3. Split `\(\{1,\ldots,n\}\)` into `\(l\)` "blocks" that don't overlap.

Why?
1. Convergence rates are slower for SGD than GD.
2. Batches improves this.
3. SGD is computationally more efficient in per-iteration cost, memory
4. Efficient when far from the optimum, less good close to the optimum (but statistical properties don't care when you're close)


---

## Returning to our example


```r
Xy &lt;- crossprod(X,y) / n # depends on the sum
XX &lt;- crossprod(X) / n # depends on the sum
maxit &lt;- 200
gamma &lt;- .05
bhat &lt;- matrix(0, p, maxit)
grad &lt;- function(b) - (Xy - XX %*% b) # no longer depends on the sum
for (i in 2:maxit) {
  g &lt;- grad(bhat[,i-1]) # no longer depends on the sum
  prop &lt;- bhat[,i-1] - gamma * g
  prop[prop &gt; 1] &lt;- 1 
  prop[prop &lt; -1] &lt;- -1
  bhat[,i] &lt;- prop
  if (sum(g^2) &lt; 1e-4) break
}
```


---

## What about the naive version?


```r
maxit &lt;- 200
gamma &lt;- .05
bhat &lt;- matrix(0, p, maxit)
grad &lt;- function(b) - t(X) %*% (y - X %*% b) / n # depends on the sum
for (i in 2:maxit) {
  g &lt;- grad(bhat[,i-1]) # also depends on the sum
  prop &lt;- bhat[,i-1] - gamma * g
  prop[prop &gt; 1] &lt;- 1 
  prop[prop &lt; -1] &lt;- -1
  bhat[,i] &lt;- prop
  if (sum(g^2) &lt; 1e-4) break
}
```

* Better to avoid the repeated calculations than to use SGD
* But have to load all the data into memory.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "ocean",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
