---
title: "04 Gradient descent"
author: 
  - "STAT 535A"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE}
source("rmd_config.R")
```

## Last time


$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmin}[1]{\underset{#1}{\textrm{argmin}}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\indicator}{1}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\dom}[1]{\textrm{dom}\left(#1\right)}
\newcommand{\st}{\;\;\textrm{ s.t. }\;\;}
$$

.pull-left[
A convex optimization problem 
$$
\begin{aligned}
\min\limits_{x \in D} &\quad f(x) \\
\text{subject to} &\quad g_i(x) \leq 0 & \forall i \in 1,\ldots, m \\
&\quad A x = b 
\end{aligned}
$$
where $f, g_i$ are convex and $D = \dom{f} \cap \dom{g_i}$.

]

--

.pull-left[


* Saw transformations.

* First Order Condition (FOC)

]


--

.pull-right[

**Relations**

.center[
![:scale 80%](gfx/cp-hierarchy.png)
]

]


---


## Very basic example

.pull-left[
Suppose I want to minimize $f(x)=(x-6)^2$ numerically.

I start at a point (say $x_1=23$)

I want to "go" in the negative direction of the gradient.

The gradient (at $x_1=23$) is  
$f'(23)=2(23-6)=34$.

OK go that way by some small amount: 

$x_2 = x_1 - \gamma 34$, for $\gamma$ small.

In general, $x_{n+1} = x_n -\gamma f'(x_n)$.

]
  
.pull-right[  
  
```{r}
niter = 10
gam = 0.1
x = double(niter)
x[1] = 23
grad <- function(x) 2*(x-6)
for(i in 2:niter) x[i] = x[i-1] - gam*grad(x[i-1])
```

```{r echo=FALSE, fig.height=4,fig.width=4,fig.align='center'}
ggplot(data.frame(x=x, y=(x-6)^2)) + geom_path(aes(x,y)) + 
  geom_point(aes(x,y)) +
  theme_cowplot() + coord_cartesian(xlim=c(6-24,24),ylim=c(0,300)) +
  geom_vline(xintercept = 6, color=red,linetype="dotted") +
  geom_hline(yintercept = 0,color=red,linetype="dotted") +
  stat_function(data=data.frame(x=c(6-24,24)),aes(x), fun=function(x) (x-6)^2, color=blue, alpha=.4) +
  ylab(bquote(f(x)))
```
]

---

## Today

1. Gradient descent implementation

2. Techniques for increasing speed of convergence

3. What if $f$ isn't differentiable?

---

## First order methods

For simplicity, consider unconstrained optimization 
\begin{equation}
\min_x f(x)
\end{equation}
assume $f$ is convex and differentiable

__Gradient descent__

-   Choose $x^{(0)}$

-   Iterate $x^{(k)} = x^{(k-1)} - \gamma_k\nabla f(x^{(k-1)})$

-   Stop sometime

---

## Why does this work?

__Heuristic interpretation:__

* Gradient tells me the slope.

* negative gradient points toward the minimum

* go that way, but not too far (or we'll miss it)

__More rigorous interpretation:__

- Taylor expansion
$$
f(x_0) \approx f(x) + \nabla f(x)^{\top}(x_0-x) + \frac{1}{2}(x_0-x)^\top H (x_0-x)
$$
- replace $H$ with $\gamma^{-1} I$

- Choose $x=x^{+}$ to minimize the quadratic approximation:
$$
0\overset{\textrm{set}}{=}\nabla f(x_0) + \frac{1}{\gamma}(x-x_0) \Longrightarrow x = x_0 - \gamma \nabla f(x_0)
$$


---

## Visually

```{r, fig.height=6, fig.width=12, fig.align="center", echo=FALSE}
f <- function(x) (x-1)^2*(x>1) + log(1+exp(-2*x))
fp <- function(x) 2*(x-1)*(x>1) -2/(1+exp(2*x))
quad <- function(x, x0, gam=.1) f(x0) + fp(x0)*(x-x0) + 1/(2*gam)*(x-x0)^2
x = c(-1.75,-1,-.5)

ggplot(data.frame(x=c(-2,3)), aes(x)) + 
  stat_function(fun=f, color=blue) + 
  theme_cowplot() + 
  geom_point(data=data.frame(x=x,y=f(x)),aes(x,y),color=red) +
  stat_function(fun=quad, args = list(x0=-1.75), color=red) +
  stat_function(fun=quad, args = list(x0=-1), color=red) +
  stat_function(fun=quad, args = list(x0=-.5), color=red) +
  coord_cartesian(ylim=c(0,4)) + ggtitle(bquote(gamma==0.1))
```



---

## What t?

What to use for $\gamma_k$? 

__Fixed__

- Only works if $\gamma$ is exactly right 
- Usually does not work

__Sequence__ 

$$\gamma_k \quad \st \sum_{k=1}^{\infty} \gamma_k = \infty ,\quad
              \sum_{k=1}^{\infty} \gamma^{2}_k < \infty$$

__Exact line search__

- Tells you exactly how far you want to go
- At each $k$, solve
$$\gamma = \arg\min_{s >= 0} f\big( x^{(k)} - s f(x^{(k-1)})\big) $$
- Usually can't solve this.


---


## Backtracking line search

Approximation to exact line search


1. Set $0 <\beta < 1 , 0 < \alpha <\frac{1}{2}$

2. At each $k$, set $\gamma=\gamma_0$. While
$$f\left(x^{(k)} - \gamma \nabla f(x^{(k)})\right) > f(x^{(k)}) - \alpha \gamma \norm{ f(x^{(k)})}^{2}_{2}$$
set $\gamma \leftarrow \beta \gamma$ 

3. $x^{k+1} = x - \gamma \nabla f(x_k)$ 


---

$$ f(\beta_1,\beta_2) = \beta_1^2 + 0.5\beta_2^2$$

```{r, message=FALSE, echo=FALSE, fig.align='center', fig.height=6, fig.width=8}
library(tidyverse)
df = expand.grid(b1 = seq(-1,1,length.out = 100), b2 = seq(-1,1,length.out = 100))
df = df %>% mutate(f = b1^2 + b2^2/2)
library(cowplot)
g = ggplot(df, aes(b1,b2)) + geom_raster(aes(fill=log(f))) + theme_cowplot(24) + scale_fill_viridis_c()
g
```

---


```{r, message=FALSE, echo=TRUE, fig.align='center', fig.align='center'}
beta <- matrix(0,40,2)
beta[1,] <- c(1,1)
grad <- function(beta) c(2, 1) * beta
gamma <- .1
for (i in 2:40) beta[i,] <- beta[i-1,] - gamma * grad(beta[i-1,])
```

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.width=8}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) +
  geom_point(data=b,aes(b1,b2),size=2)
```

---

```{r, message=FALSE, echo=TRUE, fig.align='center'}
beta <- matrix(0,40,2)
beta[1,] <- c(1,1)
gamma <- .9
for (i in 2:40) beta[i,] <- beta[i-1,] - gamma * grad(beta[i-1,])
```

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.width=8}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) + 
  geom_point(data=b,aes(b1,b2),size=2) + 
  coord_cartesian(c(-1,1),c(-1,1))
```

---


```{r, message=FALSE, echo=TRUE, fig.align='center'}
beta <- matrix(0,40,2)
beta[1,] <- c(1,1)
gamma <- .5
for (i in 2:40) beta[i,] = beta[i-1,] - gamma / sqrt(i) * grad( beta[i-1,])
```

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.width=8}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) + geom_point(data=b,aes(b1,b2),size=2)
```

---


.pull-left[
```{r, warning=FALSE, echo=TRUE, fig.align='center'}
fb <- function(x) drop(c(1,0.5) %*% x^2)
gamma0 <- .9
alpha <- 0.5
tau <- 0.9
backtrack <- TRUE
for (i in 2:40) {
  gamma <- gamma0
  while (backtrack) {
    gamma <- tau*gamma
    gg <- grad(beta[i-1,])
    prop <- beta[i-1,] - gamma * gg
    backtrack <- (
      fb(prop) > fb(beta[i-1,]) - 
        alpha * gamma * sum(gg^2)
    )
  }
  beta[i,] <- prop
  backtrack <- TRUE
}
```
]

.pull-right[


```{r, echo=FALSE, fig.align='center'}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) +
  geom_point(data=b,aes(b1,b2),size=2)
```

]


---

## Convergence properties

If $\nabla f$ is Lipschitz, use fixed $\gamma$

1. GD converges at rate $O(1/k)$
2. $\epsilon$-optimal in $O(1/\epsilon)$ iterations

If $f$ is strongly convex as well

1. GD converges at rate $O(c^k)$ ($0<c<1$).
2. $\epsilon$-optimal in $O(\log(1/\epsilon))$ iterations

We call this second case "linear convergence" because it's linear on the $\log$ scale.

---

## Momentum

Carry along some information from the previous iterate, (usually $\eta=0.9$)

.pull-left[

$v \leftarrow \eta v + \gamma \nabla f(\beta)$

$\beta^+ \leftarrow \beta - v$


```{r, message=FALSE, echo=TRUE, fig.align='center'}
beta[1,] <- c(1,1)
gamma <- .2
eta <- .9
v <- 0
for (i in 2:40) {
  v <- eta * v + gamma * grad(beta[i-1,])
  beta[i,] <- beta[i-1,] - v
}
```

]

.pull-right[

```{r, echo=FALSE, fig.align='center'}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) +
  geom_point(data=b,aes(b1,b2),size=2) +
  coord_cartesian(c(-1,1),c(-1,1))
```
]

---

## Nesterov's version (sometimes called "acceleration")

Idea is to let the gradient depend on the momentum as well

.pull-left[

$v \leftarrow \eta v + \gamma \nabla f(\beta-\gamma v)$

$\beta^+ \leftarrow \beta - v$


```{r, message=FALSE, echo=TRUE, fig.align='center'}
beta[1,] <- c(1,1)
gamma <- .2
eta <- .9
v <- 0
for (i in 2:40) {
  v <- eta * v + gamma * grad(beta[i-1,] - gamma * v)
  beta[i,] <- beta[i-1,] - v
}
```

]


.pull-right[

```{r, echo=FALSE, fig.align='center'}
b = tibble(b1=beta[,1],b2=beta[,2])
g + geom_path(data=b,aes(b1,b2)) + geom_point(data=b,aes(b1,b2),size=2) +
  coord_cartesian(c(-1,1),c(-1,1))
```
]

---

## Subgradient descent

What happens when $f$ isn't differentiable?

A __subgradient__ of convex $f$ at a point $x$ is any $g$ such that
$$f(y) \geq f(x) + g^\top (y-x)\quad\quad \forall y$$

- Always exists
- $f$ differentiable $\Rightarrow g=\nabla f(x)$ (unique)
- Just plug in the subgradient in GD
- $f$ Lipschitz gives $O(1/\sqrt{k})$ convergence

---

## Example (LASSO)

$$\min_{\beta} \frac{1}{2n}\norm{y - X\beta}_2^2 + \lambda\norm{\beta}$$

- Subdifferential (the set of subgradients)

$$\begin{aligned}
g(\beta) &= -X^T(y - X\beta) + \lambda\partial\norm{\beta}_1 \\
& =-X^\top(y - X\beta) + \lambda v \\
v_i &=
    \begin{cases}
        \{1\}      & \quad \text{if } \beta_i > 0\\
        \{-1\}      & \quad \text{if } \beta_i < 0\\
        [-1,1]     & \quad \text{if } \beta_i = 0\\
    \end{cases}
\end{aligned}$$

So $\textrm{sign}(\beta) \in \partial\norm{\beta}_1$

